Keyless table
Beginning in H2201, all keyless tables in the standard Mfg namespace, as listed below, can optionally be configured to have key fields. These tables, as well as standard tables that contain vector table, are the only standard RapidResponse tables that you can add or modify key fields for.  Previously this could only be done for four specific keyless tables (Allocation, BillOfMaterial, OnHand, and Operation).
 

Keyless tables in Mfg namespace:
Allocation
AlternateRouting
BaselinePlannedOrder
BillOfMaterial
Capacity
CollaborativeForecast
ConstraintAvailable
HistoricalPartKPI
Operation
OnHand
PoolMap
PoolMapOverride
ProductionGroupRelationship
TimePhasedSafety
ToleranceProfileZone



Issue:The DataUpdate process is continuously pulling previously processed data, while new data from the Extract folder is not being processed.
Cause
Files remained in the 'Working' folder, preventing proper processing.
The system continued to pick up and move old files to history, leading to repeated failures and data inconsistencies.
The Working folder did not clear automatically, causing the DataUpdate process to reuse previous data.



`````````````````````````````````````````````````````````````````````````````````


Article Summary
When processing multiple files across Data Sources that update a specific table in a single Data Update task, there is an order of processing which then determines which records are considered duplicated and are rejected.

The processing steps are as follows:

Move all Data Source folders to \Working directory.
Begin processing Tables in priority order.
If multiple files update a specific table, the data source that comes first alphabetically is matched and processed. Subsequent files that modify that table will be considered duplicates
For example, hen the Data Update begins updating the ReferencePart table, it processes the contents of the ReferencePart.txt file from the DataSource_A data source, then moves on to processing the ReferencePart_A.txt file in the DataSource_B data source. When the Data Update is processing the contents of the ReferencePart_A.txt file, it sees that the recordIDs have already been scanned for changes, and then considers each individual record as duplicates.
If there was a new record in the ReferencePart_A.txt file, it would insert the record in the ReferencePart table, provided auto-creation of records is allowed.
If each data source file had separate recordIds, ie: reference parts a-z in one file and reference parts aa-zz in another file, all records would be updated, as the contents of the files are unique to the data source. 

``````````````````````````````````````````````````````````````````````````````````




Article Summary
What is the correct setup to ensure that each data source has its own data update job, where each job runs specifically for its assigned data source and only imports data for that source?

Article Resolution
When any data update task is run, it will process all files in the Extract folder, regardless of their data source, and ingests the data into the mapped locations.

To achieve a different data update task for each individual data source, only the data files associated with the specific data source for the next data update task can be sent for consumption. Otherwise, any remaining files for future updates would be ignored because the system would consider the folder as already processed, treating those files as outdated.

For example:

Send files only for Data Update Task 1.
Once Task 1 completes, send files only for Data Update Task 2.
After Task 2 completes, send files only for Data Update Task 3, and so on.
This approach ensures the correct files are used for the corresponding tasks. However, it requires significant planning and testing to determine the timing of each step and avoid overlap between updates.

If assistance is needed, reach out to your Account Manager to discuss involving our Professional Services team for help with redesigning the solution.

Alternatively, you may want to submit an enhancement request detailing how you would like the data update process to work.  Before creating a new request, check to ensure a similar request has not already been submitted. 


```````````````````````````````````````````````````````````````````````````````````````````

Article Summary
We need to update some parts, but the information is not contained in one file. The data is in two files, in different data sources. Is it possible to update the same parts, using these files, from two different data sources?

Article Resolution
Updating the same part from two different data sources, using Data Updates, will not work. When the second file is consumed, it will change the values the first file inserted.

 

However, you can use a join file to update the Part fields that are not in the first file. The file Join must be in the same data source that the first file referenced. Then, create a custom table to load the data from the second .tab file and use transformations in down stream scenario to update the part table.



``````````````````````````````````````````````````````````````````````````````````````````

Configuring Tables to Allow Insert and Modify on Data Update
Applicable To: All Releases
 

Synopsis
For some tables in RapidResponse, it is recommended to configure them to only allow insert and modify during the Data Update process, instead of insert, modify and delete, in order to prevent data from being deleted.  

Typically, these are tables used to store historical data, and we want to keep previously loaded data.  In addition, some tables not used to store historical data, such as Part and PartCustomer, are required for historical data to persist and should be configured to insert and modify only to avoid unintentional cascading deletes.  This configuration is done by a data or system administrator user, who has the authorization to modify the integration settings of the system.

``````````````````````````````````````````````````````````````````````````````````````````

Preventing and Recovering from Accidental Record Deletions
 
Applicable to: All Releases
 

Synopsis
The best practice is to put procedures in place to prevent accidental data deletion, and to respond to the situation where data recovery may be required. Recovering from accidental record deletions in the RapidResponse database is very difficult because it's a versioned database and changes are tracked per scenario. Deletions are hard to diagnose, and the recovery process is tedious and disruptive. The best remedy is prevention.


`````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Symptom
Data in table  abc and xyz  was loaded as part of data update, when another file for table lmn containing reference to abc and xyz are loaded data droppages are observed saying abc and xyz is not available even though it got loaded in the same data update.

Cause
RapidResponse updates data based on the design of the data model. Table lmn was handled prior to tables abc and xyz, pertaining to the stated issue. Since table lmn is processed first, it searches for related values in tables abc and xyz, which are absent until values in tables abc and xyz are processed.




````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Field was removed from Data Sources and Mapping Workbook to stop updates from occurring on that field.
Data for the field, in RapidResponse, is now getting zeroed out each time a Data Transfer is done, instead of the values staying the same from the last data update with it mapped.
 

CAUSE:

Whether a field has been mapped and removed or never mapped at all, RapidResponse will treat it the same. RapidResponse will determine what value it should use for that field that is not mapped. It will look to see if there is a default value set. If there is no default value, then it will zero it out.

Article Resolution
If the data is saved in a Historical scenario...

 

Commit the data from the Historical scenario to Approved Actions, which is where that field's data will be maintained.

 

Since that field's data will be zeroed out in Enterprise Data, it will not see any differences from what is in the data file, so it will not push any changes down to Approved Actions, leaving the data in Approved Actions alone.

 

The only problem will be when/if there are differences, for existing records, in the data file and Enterprise Data. When this happens, it will update the record in Enterprise Data and push that changed record down to Approved Actions, which will zero out that field. To prevent this, set up a process that will update that fields values in Approved Actions, in case any records get zeroed out, due to a record update in Enterprise Data.


`````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
During a DataUpdate, records were deleted in a table as they were not included in the Data Update files. However, the records in other tables that refer to the deleted records did not generate missing reference errors. Why?

Article Resolution
There is an order of operations that DataUpdate task goes through.

Inserts and Modifies.  Before a record can be inserted or modified it will need to resolve references.
Resolve references: There are some subtle things how reference fields are treated related to the properties of the reference field: 'Allow null reference', 'Retain records in this table when a referenced record is deleted". And also how null references are resolved (ie. DataUpdate.ExtractNullRef.UseAllKeys)
Deletes. Records with the same Core::OriginatingFile value, which were not seen in the extract .tab file, will get deleted.
It is important to note that DataUpdate processes the deletes last (after the inserts and modifies). Therefore, technically the record(s) is still present to allow the reference until the very end of the DataUpdate task.

````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Manual Pooling\Pooled Batches are being deleted when keying the OnHand table and running an update task, if non-key fields are placed before key fields. This requires rearranging fields in every instance where the OnHand table is used. This article explains the necessity of key/non-key field arrangement and provides an alternative if keying the OnHand table is not ideal for your scenario.

 

Cause
The issue arises because keying the OnHand table globally impacts processes like the update task, requiring that key fields are ordered before non-key fields. When the OnHand table is unkeyed, the arrangement of fields does not matter. However, after keying the table (e.g., using fields such as Batch, Basekey, Location, Model, Part, and Type), the system expects key fields to precede non-key fields, which can impact downstream tasks and may lead to errors if the arrangement is not adjusted across all workbooks.

Article Resolution
Key/Non-Key Field Arrangement:
This keying requirement is global, meaning it affects both the update task and related processes. Ensure that, in any instance where the OnHand table is used, you arrange key fields before non-key fields to avoid issues during task execution.

 

Alternative Solution Without Keying the OnHand Table:
If keying the OnHand table does not suit your needs, you can use an alternative approach to manage record updates:

Create a Post-Data Update Process: Set up a process to update the Pool reference back to the last known value before records flow into the planning scenario.
Compare Records Using Specific Fields: Use fields such as Basekey, Batch, Location, Model, Part, and Type for record comparison to ensure data accuracy without the need for keying.
Implement Custom Process: This alternative requires a custom process to manage records without depending on key fields in the OnHand table.
 

If you need assistance with this setup or require further clarification, contact Kinaxis Customer Support.

 

Additional Information:
 

For unkeyed tables like the OnHand table, records may be deleted and re-created during data updates, especially if the Date field is modified. This can result in manual allocations being unpooled, as observed by a customer whose data became unpooled after weekend reinitializations. A custom solution to maintain record consistency across updates may help retain pooled allocations.

`````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
RapidResponse 2014.4 introduced vector series data in standard and custom tables. If you are using a RapidResponse H Service Update, you cannot convert tables to contain vector data, and you must apply the vector conversion before migrating to the RapidResponse H architecture. Converted tables must contain key fields and uniquely identify records.

 

You might see the "Key Field" option is greyed out while trying to perform this conversion process:

 

image.png

Article Resolution
This option will remain greyed out till the vectorization is applied. Before implementation of vectorization, you must determine whether adding key fields to the table results in duplicate key values. If yes, those should be either merged or cleaned up. After the Vector implementation, you will then have that option (key field) available to select.


`````````````````````````````````````````````````````````````````````````````````````````````````


While configuring RapidResponse to auto create records may seem to be an easy and helpful process, it can have some serious consequences and should be done sparingly.  Tables that allow auto create can grow quite large over time and severely impact system performance.


`````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
What is the maximum number of keys allowed for a table in data model?

Article Resolution
The maximum number of key fields allowed for a table is 8. 


```````````````````````````````````````````````````````````````````````````````````````````````````

While trying to test data update, we hit an error on the Part Table - getting message, "Ignoring Field Mfg::Part.Name from extractbecause it is ambiguous. Mfg::Part has multiple fields with different namespace with that name. Is this a softwarebug?

Answer: My guess is that someone has added another Part.Name field, using another namespace in your RapidResponse instance. So, try Mfg::Part.Mfg::Name to completely specify the table and field.

```````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
A Scheduled task is failing because a dependent scheduled task cannot update the records. The encountered error message is below: 

Cannot resolve reference field *** in column '***' to database 
table *** with key Value='***

Cause
This issue may have been caused by a scheduled task containing an incorrect workbook reference. For example, if the workbook was updated, but the scheduled task was not refreshed to reflect the update, it is possible that it could still be using outdated information.

In this instance, the execution did not produce the desired outcome. However, after reviewing the task and making minor adjustments, the private task was successfully executed.
 
Article Resolution
Complete the following steps to resolve this issue:

Check out the scheduled task that is having trouble with updating records 
Revise/Update the author's notes
Save the scheduled task and check it back in. 
The scheduled task will execute correctly with the correct context.


````````````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
When adding a custom field to an existing vector table or extending fields through a custom namespace, a live transform did not apply the data model changes.

 

Cause:
A restart is always required for existing vector tables or when extending fields through a custom namespace. Live transform does not support vector tables and fields and is not possible in these situations.


`````````````````````````````````````````````````````````````````````````````````````````````````````````

Data update failed in HistoricalDemandHeader
and HistoricalDemandSeries table
Hi All,

 

I am trying to do data update in HistoricalDemandSeries,HistoricalDemandHeader table

by setting data sources and mapping. But no data is getting inserted into the table as well as not getting any error. Even the file is not getting generated

resolution: The fields in HistoricalDemandHeader are Category and PartCustomer. These fields in turn reference to the tables HistoricalDemandCategory and PartCustomer respectively. And PartCustomer has reference to Customer.

 

We should make sure you have the values setup in the reference tables, before inserting record into HistoricalDemandHeader table. Or alternatively you may try to set the AutoCreate parameter to 'Yes' in Datasources & Mapping.


``````````````````````````````````````````````````````````````````````````````````````````````````````````

Vector sets cannot contain Notes or Set fields, which means other tables cannot contain references to the vector set. In addition, perspectives cannot be applied to the vector data.


````````````````````````````````````````````````````````````````````````````````````````````````````
Notes Field 
Notes tables are responsible for storing note data. Note data includes a timestamp, a user ID, and a value


The following RapidResponse tables have Note fields

EngineeringChange
ForecastItemParametersOutlier
IndependentDemand
ScheduledReceipt
SupplyAllocation
SupplyOrder
Task
When a note value is entered into a Note field located in one of these tables, RapidResponse creates a note record and stores it in the corresponding Note table. For example, when a note is created for the IndependentDemand.Notes field, RapidResponse creates a note record and stores it in the IndependentDemand_Notes table

 

Notes field in Custom Table
If a Note field is added to a custom table, then RapidResponse creates a corresponding Note table. For example, assume a Note field named Notes is added to a custom table named Location. The name of the corresponding note table that gets created is Location_Notes. 



`````````````````````````````````````````````````````````````````````````````````


Article Summary
When trying to get the results from dependent consensus forecast, by making an adjustment in the consensus demand workbook (using forecast detail table) the dependent forecast only gets calculated the first time an update is made at the assembly, generating dependent forecast for the components, but the second (or any other change) time, the system won't update the calculated table.

 

DependentConsensusForecast table

 

Cause
 

Kinaxis has identified this issue as a defect in the current version of the software

When we make adjustments to ConsensusForecast for parent Assembly in a BOM relationship, the changes reflect on Component in *DependentConsensusForecast *for the first time. In the successive attempts, these adjustments don't reflect on the component.
 

Article Resolution
Applying one of the Service Updates (listed below) for your version of Kinaxis Maestro (formerly RapidResponse) resolves this issue.

Available updates for all versions of Maestro can be found by selecting your version from the Downloads page (https://knowledge.kinaxis.com/s/downloads).
Available updates for all Predefined Resources (PDR's) can be found within your Maestro version as a file attachment.


KINAXIS MAESTRO VERSION: TBD
SERVICE UPDATE STATUS: TBD

 

Suggested workaround

The workbook created is based on DependentConsensusForecast table.  This table is calculated based on Forecast Details and Consensus Forecasts. There is no dependency on these results by any other planning therefore they are generated once and left as-is.

To see the results you have to clear the cache each time you make a change. You can implement a workbook command or a script to clear the cache. It is recommended to clear the cache only after making all changes you plan to make and perform this operation as few times as possible.

 

In broad terms:

Create a 'dummy' worksheet in your private workbook.
Use the 'dummy' worksheet create a new command to modify records.
Add the 'dummy' table to the 'Control Tables' workbook, 'Part Type' worksheet.
Make your planner overrides in the customer workbook.
Run the command to see the changes in your custom workbook.
 Create new dummy worksheet

Create a new worksheet in your custom workbook

Name: ‘dummy’

Table: PartType

 

In the columns tab add key fields ‘ControlSet’ and ‘Value’, add field ‘Description’ and new column ‘NewDescription’

In the data tab enter these expressions or use the ‘expression builder’ button.

Control Set>                    Expression: ControlSet.Value

Value>                               Expression: Value

Description>                    Expression: description

NewDescripition >          Expression: Text(NUMERICVALUE(Description)+1)

 

The expression in the ‘dummy -worksheet properties’ is a counter for the NewDescription Column

In the ‘Contro Tables’ add a new record for the ‘dummy’ worksheet and enter a numerical value in the description

 In the ‘Data Options’ tab check the option ‘Override default data editing permission for the ‘Control set’ and ‘Value’ columns.

 Modify worksheet data commands to include the dummy table

 

Reported Version
H2408.1
Reference Code
RRP-225466





```````````````````````````````````````````````````````````````````````````````````````````

Data Update failing with "The data update has failed with return code: -102"
Article Summary
You will see the data update failing with the following errors in the Data Import and Update log :

Unable to commit the data update working scenario () after a successful data update. Return code = 23

The data update has failed with return code: -102
Wrap Text

Active

KB.png

 

Cause:
Too many sites are being inserted/modified exceeding the licensed amount. Typically this is caused by Auto-Created records or if the LicenseType field on the Site table is not defined in DataSource and Mapping, it will supply a default value of Physical.

Note: Best practice is to avoid auto-creating site records as they are under licensing. The license violation error is only for the root (Enterprise Data) scenario.

Article Resolution
Auto-Created Record Issue:

Compare the "Total Records in Table" for table "Site" in "Record Count by Table" worksheet (Data Import and Update Log workbook) with the total licensed sites. 
Determine the auto-created fields and the associated data file using the example below and ensure that the Site table does not allow auto-created records or that the records being supplied contain the expected data



Default Site Issue:

The DataUpdate will show that records had been inserted/modified and Data Source and Mapping have nothing defined for the 'LicenseType' field on the 'Site' table:image.png
This field will need to be configured in Data Source and Mapping for the extract file updating the Site table; otherwise, all records will be inserted or modified to the default LicenseType value, which is Physical.


`````````````````````````````````````````````````````````````````````````````````````````````

Overview
As of May 2021 (release H2105), RapidResponse customers can take advantage of new features to the underlying security model that provide the ability to restrict access to sensitive data at the table and field level.


How this can benefit you
Currently, if a user does not have authoring permissions, they can only see information through workbooks shared with them. The author of such workbooks can control both row and field level content that is made visible through design. In some cases, however, data restriction may be required even for those with authoring permissions. Leveraging the new permissions management view, data admins can now set field and table-level access to users & groups and manage the inheritance of those permissions.

Table and field level security will:

Secure data and minimize the impact to end users

Allow more users to become authors

Reduce some authoring complexity to control data access.
NOTE Authors will be able to modify workbooks that use data which they cannot see, however, when they switch to runtime more, they will encounter an error message

This enhanced platform capability is applicable to all current market segments and current customers.


How to take advantage of this new feature
This feature is currently offered in Limited Availability (LA). If you have applied H2105 (and beyond), but do not have access to this capability and would like to leverage it, you will need to create a Support case to request access.



````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
In the 'Errors and Warnings' worksheet of the 'Data Import and Update log' the following error is observed:

Duplicate record for <Namespace>::<TableName> ignored with keys: <Field>=<Value>
Wrap Text

Active
 

Cause
Multiple records in the same Data Update are trying to be inserted with the same key values

Article Resolution
Inspect the record number and field values in the Data Import and Update log errors. These will point to the record line in the extract file and the field values causing the error.
Data Source and Mapping will indicate the key fields and their columns for the extract file.
Remove any records with duplicate key values in the extract file/s.
Once all records containing duplicate keys are removed, the record should insert fine.
Follow up with your source system to understand why records with duplicate keys are being extracted to avoid this behavior in the future.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Getting the following error when importing Integration Package (IPK) file

The following Tables or fields are not in the active data model, data for these tables or fields will not be imported
Wrap Text

Active
 

error

Cause: IPK file was generated on a higher version of RapidResponse and imported on a Lower version of RapdResponse

Article Resolution
Make sure the source and destination version of RapidResponse matches when importing the IPK file.

``````````````````````````````````````````````````````````````````````````````````````````````````````````````

Java Script Exception 'rapidResponse.workbooks.forEach'
Hello

 

I am working on some automated user and user group creation/maintenance for a client and there is some permission data that is not available in the 'Rapid Response System Data'. workbook.

 

In the past i have used a script to fetch resource permission data and push to a table.

 

This method is no longer working for workbooks, it does work for every other resource type (Forms, filters, dashsboard...)

 

I have replicated this error on a client server and sandbox server, it's does give different server event log errors in each system

Dev-Value cannot be null (H2503.2)
Sandbox Invalid bucket interval calendar (H2503.2).
 

I suspect that the access control list is now different for workbooks

 

Is there another way to use the foreach collection for workbooks?

 

This script generates errors in the server event log i have attached as a file here as well.

 

function main() {

  // Script execution starts here.

  // Remove this function if this script is intended to be included in other scripts.

  var resourcePermissions = new Array();

   

  rapidResponse.workbooks.forEach(function processResource(resource) {

    resource.accessControlList.forEach(function savePermission(permission) {

      var permissionRow = [permission.id, resource.name, resource.scope, permission.level, permission.isInherited];

      // resourcePermissions.push(permissionRow); // Uncomment if you want to store the permission data

      rapidResponse.console.writeLine(permissionRow);

    });

  });

}

 

Here is the console output.

 

The 'test workbook' script did not complete.

 

Line 6, char 29: UnexpectedError

 

WorkbookCollection.forEach: An unexpected error occurred.

 


Resolution:the support team identified the issue, there was a macro with a null expression that was triggering the unexpected error.

 

After maintaining an expression for the Macro the script completed.

 

```````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
What is the purpose of the "WebServerUrl" field in RapidResponse?

Article Resolution
In the Web server URL box, type the address of the RapidResponse Application Server. This is the address that users use to sign into RapidResponse. For example, https://Gateway/Comp_id.

This address is used to support the functionality that send links to resources, reports, and web addresses using Message Center and email messages.

This field is also used to open Web Client directly. In that case, the URL should be Https://Gateway/web/Comp_id/#. 

 

More information can be found here: https://help.kinaxis.com/20162/GlobalHelp/default.htm#../Subsystems/Ad/content/rr_admin/system_management_(op)/configuring_e_mail_support.htm

```````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
When user opens the Inventory Planning workbook on their environment, The data in the Safety Stock Review worksheet does not display and generates error:

 

You cannot view data in this worksheet. Please contact your RapidResponse administrator. Problem: A composite worksheet, '1.1 Safety Stock Override - References', references a component worksheet, '1.1.1 Safety Stock', in which the worksheet filter expression, or a column expression, is not valid.

 

Cause:
 

The error is coming from the worksheet "1.1.1 Safety Stock" which has a field named "BeginDate" which is added with Variable named "$ReportingCalendar" but there is no data available for this value so it throws an error.



````````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
How to allocate hard supply to Demand?

Article Resolution
To allocate hard Supply to Demand, create a ModelRule and have it enabled (make sure it is not set to ignore). In the below scenario, the MUEPoolNettingType.ModelRule is set to Ignore, which means No model netting is performed for this part based on this document > RapidResponse Data Model and Algorithm Help (Web client) - MUEPoolNettingType table (kinaxis.com)

Upon changing MUEPoolNettingType.ModelRule set to Net and the ID/SR with the same Model for example "HardLock", the SR is pegged to the ID as expected as below.



```````````````````````````````````````````````````````````````````````````````````````````````````


Does Rapid Response have inventory rebalancing feature?
The customer expects the system to recommend a transfer order to move inventory from one warehouse which has excess inventory to another warehouse whose inventory is low. Only when the total demand is lower than the total qty of the existing supplies in the warehouses, procurement should be triggered. 
This inventory transfer cannot be triggered by part sources as it would cause recursive part sources. So it looks like standard Rapid Response cannot support it. Does anyone have experience in the past of implementing a custom solution successfully for inventory rebalancing?

Look at the "Purchase Avoidance Opportunities" workbook. This workbook is a cross site inventory balancing view to look movement of inventory that avoids buying more (Planned PO). If you drill down on the Purchase Avoidance Opportunity by value ($), the Transfers Opportunities worksheet will show up at the bottom and you can then set up transfer there.

```````````````````````````````````````````````````````````````````````````````````````````````````


https://knowledge.kinaxis.com/s/article/Past-KEG-Event-Recordings


````````````````````````````````````````````````````````````````````````````````````````````````````

https://knowledge.kinaxis.com/s/article/DashboardAuthoring2015360850


```````````````````````````````````````````````````````````````````````````````````````````````````````


https://knowledge.kinaxis.com/s/system-admin-on-prem-on-demand-videos



```````````````````````````````````````````````````````````````````````````````````````````````


https://knowledge.kinaxis.com/s/Automation-Tasks-Videos
https://knowledge.kinaxis.com/s/responsibility-configuration-videos
https://knowledge.kinaxis.com/s/system-admin-on-demand-videos
https://knowledge.kinaxis.com/s/system-admin-on-premise-videos
https://knowledge.kinaxis.com/s/article/UpgradeBestPractices

````````````````````````````````````````````````````````````````````````````````````````````````

According to definition safety stock is a quantity of stock planned to be in inventory to protect against fluctuations in demand or supply. So it is not demand itself, it is level of inventory balance at which part should be at the specific point of time.

 

Safety stock itself is causing the additional planned order generation when there is no more demand and planned inventory is still below the given stock quantity. Such a cases can be reported out of SupplyDemand table where field DemandSource = None, SupplySource = PlannedOrder (some additional filtering, checks is needed, i.e to check if supply excess is not caused by moq / multiple qty).

 

When there is "real" demand then safety stock i causing the "expedite" of planned order the planned order due date will be earlier than the demand due date minus effective safety time. In such cases safety stock is causing the earlier due date planning but it is hard to say that planned order was generated "purely" because of safety stock existence.

 

Not sure which cases you are looking for (for your point 1) - the planned orders generated purely because of planned inventory below safety stock or planned orders expedited because of planned inventory below safety stock, but I think both cases can be reported out of SupplyDemand table.

 

For your question 2 SupplyDemand table and DemandSource = None, SupplySource = PlannedOrder plus additional filters for removing moq / qty multiple etc (if necessary). should be enough.

 

Safety Stock is not a demand itself, it is the inventory target so it is correctly not reported as the demand in RR tables, that's why (I think) it is not so easy to get this info from SupplyDemand / WhereConsumed / other tables.

 

``````````````````````````````````````````````````````````````````````````````````````````

	Article Summary
Planned order does not consume Onhand Inventory first when MLS is enabled.

Article Resolution
The commit level is 'MediumExplodePriority', which means the demands are processed by due date, instead of priority.

The whole point is that the safety stock has a pool that is NOT used by any demands, therefore, this safety stock is treated as 'excess' and therefore, it is processed last. Check MLS sequence number. If there is real demand with the same pool as the safety stock, MLS will perform demand transformation on the safety stock. However, the processing sequence depends on the due date of the read demand and commit level.

It is during Netting (MLS) that demand transformation is performed. Planned orders are generated in Netting, CTP does not generate planned orders.
````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


The following items describe what to check for when Planned Orders are late

Check if there is enough constraint capacity.
Check if PartSource's lot size and if it can be supplied on time due constraint or material availability.
Conditions for reclaim logic to kick in
OH supply
CommitLevel=High
Low priority demands ahead of high priority demands



`````````````````````````````````````````````````````````````````````````````````````````````````

https://knowledge.kinaxis.com/s/article/FAQSortSequenceforDemandsforSupplyAllocation4984

```````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
RapidResponse generate excess stock for parts with time phase safety stock setting. What is the reason that causes the excess stock?

Article Resolution
Cause:
The parts with time phase safety stock setting has high demands before the PDFDate and demands drop sharply after the PDFDate. With high safety stock and real demands between RunDate and PDFDate, RapidResponse is trying to satisfy these (basically planning early for the real demands) and it generates the planned orders to take care of that. However due to the Planning Time Fence, they can only be planned on the PDFDate and we are seeing a huge quantity on that date. After PDFDate, safety stock is dropping but demand is dropping faster to a much lower level, the buffer that was built up is not going to be consumed anymore and will remain as excess.

Resolution:
With parts with phasing out demand it would be good to carefully examine the Safety Stock settings. For example changing the starting date of the Safety Stock to be the PTFDate. MultipleQty also needs to be taken into consideration for later small demand quantities.















````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
Why is there a PlannedPO generated for 1020 Units when you can see that there is a balance of 1020 remaining at the site ?

What is this being used for?



q2.png



Article Resolution
The reason of the Planned order is because the SupplyType.ProcessingRule is set to Non-reschedulable. According to the SupplyType document, with this setting, Supplies are assumed to be unable to be rescheduled even if required earlier or later than it is currently scheduled. The ScheduledReceipt.DueDate is the date used for netting and explosion purposes. With this setting, a new planned order might be created to cover the demand instead of using the scheduled receipt (depending on when the supply is required).

 

A planned order is generated on 08-23-24 because there is a total demand of 1024 due on that day. There is an SR due on 08-24-24 but it is late by one day so Maestro creates a PO on the demand due date to provide on time supply.



To resolve issue, change the SupplyType.ProcessingRule is set to RecommendOnly. In doing this, no planned order will be generated because RapidResponse will try to use the scheduled receipt before planning any new planned orders with this setting 



```````````````````````````````````````````````````````````````````````````````````````````


https://knowledge.kinaxis.com/s/article/Planned-order-available-dates-are-future-for-an-expiry-assembly-part

````````````````````````````````````````````````````````````````````````````````````````

Article Summary
A particular part has only one transfer PartSource at site A, which is getting the transfer from site B. However this part at site B does not have any PartSources so it is not planning. While there is a break in the network, missing PartSource, the Planned order at site A is still calculating a non-Future AvailableDate. Knowing that PlannedOrders (and DueDate) are generated during the netting calculation, the expectation is that during CTP calculation the available date would be calculated as ‘Future’ because the supplying site B is missing a partsource and not planning.

How is the available date at site A being populated when the supplying site B is not planning or supplying?

Article Resolution
The current behavior is expected.

In the original design of RapidResponse core algorithms, there were 2 design choices that were consciously made, but differ from majority of other planning systems:

A Part without any valid PartSource is ignored in core algorithms. In other words, it’s equivalent to setting the Part.Type.ProcessingRule = Ignore. This means that during netting initialization, this Part record (without a valid PS) won't be processed at all.
A Make or Transfer PartSource at the bottom level of the product structure (aka, no dependent level below it) is treated as a Buy PartSource automatically. In other words, if a PartSource (that should generate dependent demand) cannot generate dependent demand (due to mis-configuration), RapidResponse assumes that the PartSource represents buying from a supplier and thus has no dependent requirement.
In RapidResponse, a Part without any valid PartSource is considered a major configuration error. If the parent part is expected to have CPOs's AvailableDate=Future due to lack of component material, the component part needs to have a PartSource with OrderPolicy.OrderGenerationRule set to NoOrders. 

Any of the following configuration will ensure a PartSource is valid:

Type.EffectivityRule = Always
Type.EffectivityRule != Never and EffectiveInDate < EffectiveOutDate
The Data Model and Algorithm Help document has been updated to explain how parts without any PartSource is treated. 

- Any Part records without a valid Part Source is treated as Ignore.
- For Make or Transfer Part Sources, if the component / sourcing site is an invalid part, the parent Part Source is automatically treated as a Buy Part Source.


````````````````````````````````````````````````````````````````````````````````````````````````````````````

Ensure that both the transfer part and sourcing part in each transfer relationship have one or more valid PartSource records defined. Any part not having at least one valid PartSource record will be ignored and not planned during Netting calculations (that is, treated the same as if PartType.ProcessingRule were set to Ignore).

In cases where the part at the sourcing site does not have at least one valid part source (for example, it does not have a PartSource record or its PartSource record(s) never becomes effective), supply of the transfer part will still be planned assuming that it has a valid part source. In such cases, however, the transfer part will not explode dependent requirements to the sourcing site and availability of the supply will be based on the transfer part's parameters only. In this sense, the transfer part behaves as if it were from a Buy source under such conditions.

------------------------------------------------------------------------------------------------------


PTFRule of LastDueLead might help. with this setting PTF will include the Lead Time.

New PTF would be the current PTF + Lead Time. startDate may fall before the new PTF (PTF gates DueDate and not StartDate) but not before your current PTF.

======================================================================================================

Article Summary
Users are noticing that maestro is generating planned orders for future demands that are outside the effectivity dates of the part sources. This is causing a conflict with substitution setups, as the planned orders should not be generated for the primary parts after the effective end date of the partsource.

 

issue 1.png

Article Resolution
The issue is caused by the SourceRule.DateEffectivityRule being set to Normal. 

As per the docs, changing the Date Effectivity Rule from Normal to AdjustedDemandDate resolves the issue - Maestro Global Help - SourceRule table
https://knowledge.kinaxis.com/s/article/Maestro-generating-planned-orders-for-future-demand-outside-date-effectivity-of-part-sources

--------------------------------------------------------------------------------------------------------------------------

RR may move ship date in the past if there is no date available in the ship calendar.

------------------------------------------------------------------------------------------------------------------------

Article Summary
Demand exceptions resulted from the configuration in the automation for ITE calculation as below:

-       PartSource.CTPITE = ITE

-       PartSourceType.ExpiryRule overwrites the PartType.ExpiryRule

-       PartType.UseDaysSupply = 0

-       Many ExpiryDate for ScheduledReceipts (SR) are not set

 

Root Cause:

1.     PartSource.SafetyLeadeTime was used

2.     PartSourceType.ExpiryDateRule = DockDate

3.     Current ITE calculation of ITE left out the SafetyLeadTime PartSourceType.ExpiryDateRule = Dockate forcing the ITE calculation to consider the SatetyLeadTime

Article Resolution
Simplify the ITE calculation:

-       Set PartSourceType.ExpiryDateRule = DueDate for all PartSources

-       Adjust the ITE for raw material accordingly

-       New IT calculation: Parent PS.ITE = Child PS.ITE – Child Part's SafetyLeadTime – Parent PS's LeadTime


`````````````````````````````````````````````````````````````````````````````````````````````````````````````````

When expiry logic is enabled, demands are forced to be processed in date sequence regardless of OrderPriority, EarliestExpiryDate (EarED) for a demand means it is the earliest date a supply is allowed to expire for it to be usable by the demand. The demand can only be satisfied by supplies that expire after its EarliestExpiryDate.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````



The way RapidResponse calculates Safety Stock considers Part.PlanningCalendar.TimeUnit. This may cause confusion when the calendar used for demand grouping (in Planning Sheet) and part's time unit are different. 

```````````````````````````````````````````````````````

The parts with time phase safety stock setting has high demands before the PDFDate and demands drop sharply after the PDFDate. With high safety stock and real demands between RunDate and PDFDate, RapidResponse is trying to satisfy these (basically planning early for the real demands) and it generates the planned orders to take care of that. However due to the Planning Time Fence, they can only be planned on the PDFDate and we are seeing a huge quantity on that date. After PDFDate, safety stock is dropping but demand is dropping faster to a much lower level, the buffer that was built up is not going to be consumed anymore and will remain as excess.


```````````````````````````````````````````````````

safety stock demand does not have a demand type of any sort (although it does have priority), the safety stock demand will always be planned for and never "expired" or ignored.

````````````````````````````````````````````````

When partial planning is enabled and an expiry demand exception is encountered, RR creates a demand exception but does not drop the demand. Traditionally partial planning only drops demand for lateness, e.g. material lateness (with MLS), constraint lateness, or PTF reason.

Article Resolution
Kinaxis Dev team has determined that resolving this issue requires deeper investigation into the interactions between several features—specifically partial planning with expiry, safety stock, and demand exceptions. Currently, these interactions are not clearly defined, and additional analysis is needed to ensure a robust solution.  

An enhancement request, RRP-234516, is generated to support a more comprehensive investigation and potential future enhancements.


````````````````````````````````````````````````````

 identify the Safety Stock Demand out of the  ​SupplyDemand table to show its impact on PlannedOrders . I think kzbikowski is absolutly right. The Safety stock is not the demand itself, but drives the demand.

 

My solution was to have a look on PlannedOrders generated out of SafetyStockQty. policy.

 

What I did was I filtered for:

 

DemandSource Like 'None'

And

SupplySource Like 'PlannedOrder'

 

in the SupplyDemand table (Allocated Qty).

 

In a second step I matched the DemandDueDate (Supply Demand table)  of the SafetyStock with the DemandDate of my 'PlannedOrder' (CTP Activity Table) .

 

All in all a quite time consuming and complicated task ;)

``````````````````````````````````````````````````````````````````````````

Article Summary
We have seen a case where the system is not handling TimePhasedSafetyStock (TPSS) correctly.

It appears that when we have a large TPSS quantity, Maestro is building to the safety stock instead of building other parts that have actual customer orders.

These FG’s have the same components so the expectation is that the system would be creating planned orders on the FG that has real demand and not create planned orders on the FG that has TPSS. 

Article Resolution
There are a few reasons that could contribute to this scenario.

 

1) Changing the commit level to MediumExplodePriority

2) If using MLS, change the MLS.ShortTermExcessRule from Reduce to Allow

3) Delay the start date of the safety stock, and/or reduce the amount of safety stock.

4) Enable the demand transformation feature i.e. changing the (SafetyStockPolicy.DemandTransformation = "FirstInFirstOut").

5) If all of the above fails and shortages are still being seen, then please engage Kinaxis Support as the overall solution may need to be reviewed prior to suggesting further recommendations.


`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
Problem
How do I set up my environment to take advantage of Order Priorities?

 

Resolution
The following is a list of fields that should be reviewed when setting up Order Priorities.
Many of these values will override other settings.

Part Type table fields:

UseDaysSupply
Valid values are:
• Y
• N

If Y (Yes), use Part.NumberDaysSupply to group planned orders. Planned orders are
grouped for the lesser of ‘n’days or until a non-reschedulable supply is seen.
MaximumQty still applies to individual PartSources. PriorityRule is treated as ‘Ignore’,
regardless of its actual value.
All supplies for parts with UseDaysSupply will be assigned priority of DefaultPriority.
CAUTION: Avoid using UseDaysSupply with parts with components or Constraint.
UseDaysSupply can cause order commitments to be violated by new demands. (Increased
supply quantity may result in a supply being late, thereby making all demands for that
supply late).
Recommendation: Use part source ship calendars, rather than days supply, to align
periodic deliveries.

If N, Part.NumberDaysSupply is ignored.
Default value: N
Note DaysSupply=’Y’ overrides CommitLevel.

CommitLevel
Controls how order commitments are maintained and sets how priority is applied to
netting (supply planning) and CTP (available date calculations).
Valid values are:

• High — priority is used in both netting and CTP. This setting ensures that priority is
propagated from demands to planned supplies. Appropriate for parts that have
constraints or whose components could be constrained. For example, this setting
would be used for purchased parts with supplier constraints, and for most
assemblies.

• Medium — priority is used in CTP, but not in netting. This means that priority is not
propagated from demands to planned supplies, nor is priority used when
allocating constraint to supplies for this part. However, priority is used when
allocating constraint to supplies for this part. Appropriate for parts that have no
dependent limitations (including no constraints) but may have limited
availability due to lead times and time fences. For example, this setting would be
used for purchased parts without supplier constraints.

• Low — priority is not used in either netting or CTP. Nor is priority used when allocating
constraint. Appropriate for parts where all part sources have no active Constraint and
where any Make sources have unlimited availability. Appropriate for parts that have
no practical limit to availability (neither constraint nor limitation on required
components).
Note also that a Low setting overrides a setting of Y on OrderPriority.MaintainCommitments.
This is the default value.

Default Priority
The priority to use for planned orders resulting from safety stock requirements or
when PriorityRule = ‘Ignore’.
Referenced table: OrderPriority.Value Default value: OrderPriority with the highest number.

OrderPriority Table
MaintainCommitments
Controls whether transaction sequence is considered as part of priority for orders using
a priority.
Values are:
• Y — transaction sequence is considered part of priority. Typically, this setting should be
used for orders that are truly committed to production (for example, the highest
priority independent demands).
• N — transaction sequence is not considered part of priority. This is the default value and
should be used for most priorities.
Note that when PartType.CommitLevel is set to Low, transaction sequence is ignored
regardless of the setting of this field.

PlanningPriority
The relative importance associated with each OrderPriority value. Lower values are
considered the higher effective priority.
Different OrderPriority records can share the same PlanningPriority value. If multiple orders
share the same PlanningPriority value, they are treated the same, even if their
OrderPriority differs. Therefore, it is recommended that you give each
OrderPriority record a distinct PlanningPriority value.

Sorting for demands:

1. DueDate (if not demand is not late)
2. Priority & Sequence
3. AllotmentOrder (from DemandType)
4. PegSupply.DueDate
5. Quantity
6. Order.Id and Order.Site (PegPart & PegSite for dependent demand)
7. OrderType
8. Line
9. Model
10. Unit

Sorting for supplies:

1. AvailableDate (or DemandDate) based on the
PartType.SupplyAllocationRule=ByAvailable,
respectively ByMRP
2. SupplyType.SortBeforeDate
3. Priority & Sequence
4. ReschedDate
5. SupplyType.SortSameDate
6. Status.SortBeforeStart
7. StartDate
8. STatus.SortSameStart
9. RecommendDate
10. EffDueDate
11. Quantity
12. Order.Id & Order.Sie
13. OrderType
14. Line
15. Pool
16. Model
17. Unit


````````````````````````````````````````````````````````````````````````````````

Article Summary
A user has a requirement to lock the sales forecast so that the current month forecast is un-editable. Currently, users can edit the forecast for all months from the current date forward. Instead, they want to make the current month un-editable while the future months (today + 1 month) are all editable.



For example, on April 2, Sales Reps need to see the April forecast quantity but should not be able to edit the April sales forecast quantity. They should be able to see and edit the sales forecast quantity for May and all months after.

Article Resolution
Conditionally editable column applies to the whole column. In a crosstab worksheet is either editable or not editable for all buckets. There is no feature to make this conditionable to the date. There is currently an Enhancement Request for this (CER-I-98).


````````````````````````````````````````````````````````````````````````

Issue:
While testing a small data change in DEV, most parts were showing zero supply (all pegged to "FUTURE"). Usual error checks showed nothing abnormal, and production data worked fine.

Key Observations:

Setting constraints to LOADONLY improved supply.

Making resources "infinite" had no effect.

Turning MLS off also improved supply.

Problem appeared only in DEV after deleting some records (BOMs, part sources, source constraints).

Suggestions (from Nancy):
Possible causes of missing supply:

Recursive BOM structure.

Missing part source.

BOM with no available constraint.

Order policy = NoOrders.

Extremely large/erroneous lead time.

Missing planning calendar dates.

Resolution:
Root cause was recursion in a BOM record. Once removed, supply returned to normal.


``````````````````````````````````````````````````````````````````````````````````````````````````

Analytics results are delivered through calculated fields, not just calculated tables. If a calculated the field is present in your worksheet, even if the column is hidden, the analytic will be run.

 

Avoiding running analytics does not make sense. How much value can you get from reporting on input data? RapidResponse hides the complex calculations behind a simple field like AvailableDate.

 

Having said this, you need to use analytics judiciously: apply proper filtering, use only what you must, use the simplest analytics table that has what you are looking for instead the more complex one, which is more expensive to calculate. Follow Pascal's advice. Start at the bottom of the composite and check if every additional step is built in an efficient way. There are a number of good Expert Series videos on the KLC (Kinaxid Learning Center) that explain factors affecting worksheet performance.

 

The first step is to look at the worksheet performance dialog. It gives you the amount of CPU time spent on analytics and the number of records and processing time. The details show you LOD (Line of Descent) which illustrates how filtering was applied.

 

Also, there are techniques that can help, such as cached worksheet. Often, doing reporting on a shared scenario that is frequently edited causes analytics cache to be recalculated all the time. In those cases, adding a reporting scenario that is updated once every 15 minutes would help.

 

Troubleshooting performance problems is not easy, but it is very much possible to achieve a 10x improvement when worksheets are not built the best way. Don't lose faith :-)


`````````````````````````````````````````````````````````````````````````````


What is the Kinaxis Maestro Database Integrity Check (DIC)?
Maestro checks the database for invalid references and inconsistent counts in scenarios. This procedure is called a Database Integrity Check and can alert you in advance of potential operational problems with Maestro.

Maestro includes a default scheduled task that runs once each day and executes the Maestro DataIntegrityCheck command.


To review the DatabaseIntegrityCheck command logs
In the Administration pane, under System Monitoring, click Administration Log
Choose the Command Messages worksheet and select the command DataIntegrityCheck from the drop-down menu
Note: The logged information in the Administration Log workbook is available to system and data administrations, including Master Admin. 



Taking a proactive approach, rather than waiting for a system problem or failure, can potentially and significantly reduce risk:

Inaccurate resource reporting
Potential performance degradation
Potential for complex data remediation
Risk to system stability
Accuracy of analytic results


``````````````````````````````````````````````````````````````````````````````````````

Topic: Retrieving column expressions in RapidResponse via scripting.

Problem:
A user created a script to list workbooks, worksheets, and columns, but also wanted to extract column expressions.

Key Responses:

Not possible via scripting API – expressions, grouping, and bucketing cannot be retrieved or authored through scripts.

Workaround suggested: Use Workbook Compare by making a dummy change or comparing to a blank workbook. The compare file contains all workbook properties (including expressions), which can then be parsed.

Confirmation: This method works; user planned to parse the compare file.

Additional note: Kinaxis internal tools and hackathon projects exist for workbook visualization, including graph views in the Web Client that show relationships and properties.

Outcome:
Direct retrieval via script is not supported, but using Workbook Compare + parsing output provides a practical workaround.



```````````````````````````````````````````````````````````````````````````````````````

Article Summary
There is a requirement to track all data changes made within the system whether through manual user edits, workbook commands, data updates, or Excel uploads. Is there a centralized table or logging mechanism that can provide a complete, end-to-end audit trail of all data modifications across different input methods?

 

 

Article Resolution
The Data Import & Update Log workbook provides visibility into changes made via automation (data updates).

For manual imports (e.g., Excel to Workbook), only summary information is available such as record count and import errors.

User changes to scenarios (via Import to Scenario) can be viewed using the Scenario Properties pane under:

Data Changes (pending commits)

Inherited Data Changes

At present, Kinaxis does not provide a unified, end-to-end audit log that captures every change across all interfaces (workbook commands, Excel imports, etc.).

If extended data retention or a full audit trail is required, it is recommended to export DataChange logs regularly to an external system before the 45-day retention limit.


````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
If there is a situation where Scheduled Receipts would have zero or negative quantity due, how would RapidResponse handle that?

Article Resolution
Records with zero or negative quantity are ignored (ScheduledReceipt.Quantity) for netting. However, the record would still be able to be seen in Rapid Response.

 

If these records are not being seen in RapidResponse, check the data update files to make sure they are being included for data updates


`````````````````````````````````````````````````````````````````````````````````````````````````````

The user wants access to all currency conversion rates from the CurrencyConversionActual system table, but cannot create a worksheet (WS) directly on it. In their current workbook, a list variable restricts data to only the selected currency, and they cannot modify or copy that worksheet.

Initially, it seemed the only way to change data in this table was through Data Update to the Enterprise scenario, but later testing showed otherwise. By copying the Currency Conversion Rates standard workbook, it is possible to edit the worksheet called Conversion Current. Removing filters and conditional hiding allows viewing all records. Alternatively, a new workbook can be created, and the worksheet copied from the standard one. After turning off GroupBy and deleting filtering, the Date and Rate fields become editable, and new records can be inserted (though not deleted).


```````````````````````````````````````````````````````````````````````````````````````````````````````

The system variable $SelectedUnitofMeasure, $SelectedUnitOfMeasureConverted
``````````````````````````````````````````````````````````````````````````````````````````````

 rapidResponse.loggenOnUser.id

``````````````````````````````````````````````````````````````````````````````````````````````

 system variable ENVIRONMENT("ServerType") 

``````````````````````````````````````````````````````````````````````````````````````````

"$Scenario#0" system variable

``````````````````````````````````````````````````````````````````````````````````````

🔹 Issue

When running Statistical Forecast Set-up in S&OP Forecast Item Management, no results appear because the system cannot find any ForecastItemParameterType record where System::IsDefault = true.

🔹 Root Cause

Newer releases (2016.2.15 and later) of S&OP Forecast Item Management depend on the System::IsDefault record in the ForecastItemParameterType table.

If multiple records exist, none may be flagged as default, causing the command to fail.

🔹 Solution

Go to Enterprise Data and review ForecastItemParameterType records.

Delete unwanted records (ensure they aren’t used in child scenarios).

Reboot the server.

The system will then set System::IsDefault = true on the remaining record.


```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

🔹 Issue

During site transformation (EBS → SAP), a customer found that calculated intersite demands differ between scenarios, even though input data counts are identical.

🔹 Root Cause

Each input record has a RecordID, a unique sequential identifier.

Site conversion requires recreating some SRs/IDs, which changes the RecordID sequence order.

Since RapidResponse calculations depend partly on RecordID sequence, differences in ordering can lead to variation in calculated demands/supply.

🔹 Resolution

The differences in calculated results are expected system behavior after a site transformation. This is due to how RecordIDs (unique identifiers for input records) are regenerated during the conversion process:

When sites are transformed (e.g., from EBS to SAP), certain supply records (SRs) and IDs must be recreated.

This recreation changes the sequence/order of RecordIDs compared to the original scenario.

RapidResponse uses RecordID sequence as one of the internal factors when pulling and calculating demands.

As a result, even if the input data matches 100% between the two scenarios, the calculation results (e.g., Intersite Demands, supply quantities) may differ slightly because the RecordID order is no longer the same.

Therefore, after performing a cross-site transformation, it is not guaranteed that calculated results will be identical between the original and transformed sites. Minor differences should be considered a normal outcome of the transformation process.


```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Article Summary
Symptom
The Bill of Material (BOM) chain is broken when a Phantom part is introduced

Cause
The BOMAlternative on the Phantom part. part sources are not populated

 

Article Resolution
Populate the BOMAlternative for the Phantom parts to match their parent's BOMAlternative value

````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
When the expiry type is set to Self/SelfReset is it also mandatory to have the Intervals To Expiry (ITE) as advised, where the assembly ITE takes component ITE and decrements the Lead time?

Article Resolution
Yes if PrePlan is enabled (AbsPPL<>0). When expiry and preplan are enabled, ITE should be set much bigger than AbsPPL regardless of ExpiryType. In this case the default ITE which is 0 will very likely cause issues.

``````````````````````````````````````````````````````````````````````````````````````````
A Part without any valid PartSource is ignored by the core algorithms. It is equivalent to setting the Part.Type.ProcessingRule = Ignore.

2
A Make or Transfer PartSource at the bottom level of the product structure is automatically treated as a Buy PartSource. If a PartSource cannot generate a dependent demand due to a misconfiguration, RapidResponse assumes that the PartSource represents buying from a supplier and thus has no dependent requirement.

``````````````````````````````````````````````````````````````````````````````````````````````

Best Practice: Be cautious when using PartSource.MaximumQty to avoid high CPO count per part

When RapidResponse stores calculated results in the algorithm cache, each calculated planned order (CPO) is stored individually. This means that 10 CPOs of 100 units each with the same DueDate occupies 10 times the memory compared to 1 CPO of 1,000 units. Core algorithm run time also increases by the same factor. In other words, higher CPO count leads to slower system performance.

```````````````````````````````````````````````````````````````````````````````````````````````````
Tip:
Perform data changes in a private scenario

In a previous lesson, you learned that any data changes, such as inserts, deletions, and modifications, can invalidate cached algorithm results. After a cache invalidation occurs, the algorithms have to recalculate planning results when the next data read is performed. 

In a standard user environment, there can be many users performing data reads in key shared scenarios frequently throughout the day. When algorithm results are cached and data reads occur, the system is fast and users retrieve planning results quickly. However, if a user or automation chain is performing data changes in the same shared scenario as other users performing data reads, the system can be much slower to retrieve planning results. As the cache can be invalidated from the data changes and the algorithms need to recalculate planning results before users are able to view them.

This means:

•
For interactive simulations, users should create a child scenario and perform their data changes within it.

•
For automation chain design, the chain should start by creating a temporary child scenario, then perform the data changes in that child scenario.



````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

CurrencyConversionActual table
Contains currency conversion rates, which are available in all scenarios. The latest rate with a conversion date on or before the current date is used to convert values. If a rate is modified, the same modification is present in all scenarios.

This table is a system table that is updated similarly to an input table. Updated rates should be brought in to this table using data updates, which makes the updated rates available in all scenarios.

If a new currency is added, you must update the data extracts and integration to include the conversion rates for the new currency, which ensures the currency returns meaningful values.

Any conversion rates specified for the base currency are ignored.

The data in this table is updated by performing data updates. However, this table is updated only if the data update provides data for the root scenario. This ensures actual conversion rates are not accidentally added. Data updates can only add records to this table.

If multiple data updates provide a record for a specific date, each of those records are added to the table. If multiple conversion rates are provided for a date, the rate that is used to perform conversions cannot be predicted, which can result in incorrect results. When you provide data for this table, you should ensure only one rate is provided for a date.


`````````````````````````````````````````````````````````````````````````````````````````````

RParameterName table
The RParameterName table is used to identify and maintain function names and the arguments that relate to each function in the R programing language.

````````````````````````````````````````````````````````````````````````````````````````````

Activity(Mfg) table
there is another Activity table, Activity table in the ProcOrch namespace, which defines all the individual work items that together form a process

Maestro includes worksheets based on the Activity table that combine all OnHand, Supply, and Demand information

These worksheets can be used for projecting inventory balance or generating a complete transaction event report.
The Activity table only presents events or activities that are processed by netting 


Generation of TimeUnit records
Maestro creates Activity records when there is a transaction to report. For example, if an unsatisfied demand is satisfied with available inventory, two Activity records are created. One that shows the demand and the other showing the supply.


#Field: AdjustedDemandDate: 	
For demands, this returns the DemandDate less the part's safety lead time (note that any safety lead time at the part source level is not included). In addition, safety stock can impact this date by expediting the demand earlier in order to fulfill safety stock requirements.

For supplies, this is the same as RecommendedDate (which considers safety lead time from both the part and part source).

#AssociatedDate:The PromiseDate for demands, CalcStartDate for supply, RunDate for on hand.

#BalanceDelta

The change in the inventory level due to the original quantity of the activity.

#DemandDate: For supplies, date of the earliest demand matched by netting. For demands, ReschedDate.

```````````````````````````````````````````````````````````````````````````````````````````````

ConsensusForecast table
The ConsensusForecast table contains the consensus forecast generated from the sales and operations planning process in Maestro. Each record in this table contains a calculated forecast amount for a part and customer combination on a given forecast date.


 The weight assigned to a particular forecast category when calculating the consensus forecast for a part and customer combination is determined by the ConsensusForecastWeight field in one of the following tables:

HistoricalDemandHeaderTimePhasedAttributes
HistoricalDemandHeaderRollingWeight
HistoricalDemandHeader
HistoricalDemandCategoryRollingWeight
HistoricalDemandCategory

Note that only categories with a ConsensusForecastWeight value greater than zero are used in calculating the consensus forecast values reported in this tabl

Consensus forecast quantities are subsequently reported in the Forecast table (subject to any forecast spreading logic) with a ForecastSource value of ConsensusForecast. Availability of the forecast is given in the CTPForecast table, and details of all supplies used to satisfy the forecast can be seen in the WhereConsumedForForecast table.


If the ForecastDetail table contains a record for a given date, part, and customer where Header.Category.Type.ProcessingRule is set to ForecastOverride or ReBalancingForecastOverride, then that record value is automatically reported in this table in the OverrideQuantity field instead of the calculated consensus forecast.

Note:ConsensusForecast records can only be generated for parts where the PartType.ProcessRule value is set to either MPS or MPSConfig.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


The user wants all conversion rates for all currencies from the CurrencyConversionActual system table, but cannot create a worksheet directly from it.

Their current workbook only shows conversion rates for a single selected currency (via a list variable), and they cannot change or copy its filtering.

Answers provided:

Officially, the only way to update the CurrencyConversionActual table is through Data Update to the Enterprise scenario.

However, it was found you can also:

Copy the Currency Conversion Rates standard workbook.

Use the Conversion Current worksheet.

Remove filtering and conditional hiding to see all records.

Optionally create a new workbook and use Copy Worksheet from another workbook.

Disable GroupBy, remove filtering, then Date and Rate become editable.

You can insert new records, but not delete existing ones.

⚠️ Caution: Direct editing is powerful but risky — you must fully understand the impact before making changes.

👉 In short: Direct WS creation from CurrencyConversionActual isn’t possible, but you can copy/modify the Conversion Current sheet from the standard workbook to access and edit rates.


`````````````````````````````````````````````````````````````````````````````````````````````

🚨 Issue

When running a Scheduled Task in RapidResponse, users encounter errors such as:

Autocreate failure: Cannot use an invalid autocreated record on table Mfg::DemandOrder.

Autocreate failure: Cannot resolve reference field Customer to database table Customer, no values specified and no default.

No changes were made to the workbook or the scheduled task before the issue appeared.

✅ Root Cause

The issue arises because of an Outer Join condition in the workbook.

Outer Join pulls all records from both tables.

This can include records with missing references (invalid or null values), leading to autocreate failures.

🛠 Resolution

Replace the Outer Join with an Inner Join in the source worksheet.

Inner Join ensures only matching records from both tables are included.

This prevents invalid or missing references and resolves the autocreate error.

👉 In short:
Switching from Outer Join to Inner Join in the workbook resolves the scheduled task autocreate failure.

```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Expiry is lost when Grp Netting Active ?
We see planned Transfer Future date and not expiry date
Group Netting Expiry Issue

I see in Control tables for PartSourceType we see the Expiry rule set with different values for similar Part Source types. For example…..

 

 

These different types of Transfer PS have different Expiry Rule settings.

One side effect of this setting I have noticed recently is related to Group Netting. Let’s say I have inventory of a part in both DE14 and DE17 which is likely to drop below the Minimum Shelf Life. Prior to GroupNetting I see the expiry risk on this inventory and supply is generated to replenish it.

However, after running the first three steps of GroupNetting, now this inventory is sitting upstream in the network in site DENet. The Planned Transfers in to DE14 and DE17 supplied by the inventory in DENet have no expiry date calculated so nothing appears to expire. This generates lower net requirements at source and lower supplies in to DENet.

 

 

 

When we reach the point where GroupNetting is removed ready to think about deploying supplies across the network, we now see negative inventory projections because the inventory has been replaced in DE14 and DE17 and we see the expiry risk again.

 

Please find the attached documents for details .


``````````````````````````````````````````````````````````````````````````````````````````````````


Capacity Allocation with Priority – Summary

Planned orders sometimes fail to respect sales order priorities in RapidResponse when using CommitLevel = High. High-priority demands may not always receive supply earlier than lower-priority demands because certain features conflict with priority processing.

Key Conflicts (must be turned off for CommitLevel=High to work properly):

DaysOfSupply → Forces CommitLevel to Low during Netting, ignoring priority.

Expiry → Demands processed by due date to minimize expiry risk.

OrderPriority.PeriodEffective=Y → High-priority demands in later periods may be delayed behind earlier low-priority demands.

OrderPriority.AllocationRule=Fairshare → Supply distributed evenly, not by priority.

How OrderPriority Works

Netting: With CommitLevel=High, higher priority demands are processed first, creating PlannedOrders that inherit the demand’s priority.

CTP Scheduling: Scheduling respects supply OrderPriority.

CTP Allotment: Allocation per Part/Site may reassign supply to reduce lateness for lower-priority orders, provided high-priority orders are not delayed.

What OrderPriority Does Not Do

It does not force supplies of a given priority to only serve demands of the same priority.

Local Allotment Example

If a high-priority demand (day 10) and medium-priority demand (day 11) are netted, CTP may reshuffle supplies (FIFO) so that the day 10 demand takes day 9 supply, and day 11 demand takes day 10 supply. While both are on time, this creates a mismatch where high-priority demand uses medium-priority supply and vice versa.

👉 In short, priority is respected in planning, but certain features and CTP adjustments can cause supply-priority mismatches.

``````````````````````````````````````````````````````````````````````````````````````````

Issue
RapidResponse allocates inventory from non-primary sources and allows lateness at downstream DCs even when stock exists at the primary source above safety stock.

Causes

Demand Sorting Sequence

Non-reschedulable SRs (LLC0) create new allocations on LLC1.

Independent demands/forecasts on LLC1 get processed first, consuming primary source SRs.

When PTAs arrive later, only non-primary sources remain available, so RR allocates from them to reduce lateness.

IntervalDate Sorting

With co-by products in a family, IntervalDate (PTFDate) is prioritized over demand due date.

PTAs inherit PTFDate, causing them to be sorted/processed later than IndependentDemands and forecasts.

Resolution / Workaround
Use RecommendOnly SRs to avoid premature allocation and allow better alignment of sourcing from primary locations.


`````````````````````````````````````````````````````````````````````````````````````

Issue
At site A, a part has a transfer PartSource from site B. However, site B’s part has no PartSources (not planning). Despite this break in the network, the Planned Order at site A still shows a non-Future AvailableDate, instead of “Future” as expected.

Explanation
This is expected behavior in RapidResponse due to core algorithm design choices:

Parts without a valid PartSource are ignored → treated as if ProcessingRule = Ignore.

Make/Transfer PartSource with no dependent demand is treated as Buy → assumed to represent external procurement when dependency cannot be generated.

Thus, site A still calculates a valid AvailableDate instead of “Future.”

Resolution

Ensure configuration is correct: if a component should prevent supply, give it a PartSource with OrderPolicy.OrderGenerationRule = NoOrders.

Valid PartSources must meet one of:

Type.EffectivityRule = Always, or

Type.EffectivityRule != Never with EffectiveInDate < EffectiveOutDate.

Documentation has been updated:

Invalid PartSources → treated as Ignore.

Parent Make/Transfer → treated as Buy if child part is invalid.

``````````````````````````````````````````````````````````````````````````````````````````````````````````

Issue
In a BOM:

Assembly A → Phantom component B → Normal components C & D
When a planned order exists for Assembly A, RapidResponse does not generate dependent demands on C & D.

Cause

Phantom part B has no PartSource.

In RapidResponse, any part without a valid PartSource is ignored in netting (treated like ProcessingRule = Ignore).

Therefore, dependent demand on B is not processed and cannot be exploded to C & D.

Resolution

Create Make PartSources for Phantom parts.

This allows dependent demand to “blow through” correctly from the phantom to its components C & D during explosion.

``````````````````````````````````````````````````````````````````````````````````````````````````````````````

Issue
PlannedTransfers to sites 1464, 1471, and 1878 (due Oct/Nov 2025) show their source SR Due Date at US_National in 2024, which looks inconsistent.

Cause

In Netting, Maestro uses the SR DueDate (2024) for planning.

However, the SupplyStatus.AvailableDateLimit for these SRs at US_National is set to Now, making their AvailableDate much earlier than the DueDate.

During CTP, supply allocation uses AvailableDate, creating a mismatch between Netting and CTP results.

Resolution

Change SupplyStatus.AvailableDateLimit from Now to CalStart so AvailableDate aligns better with DueDate and avoids discrepancies in CTP allotment.

Do you want me to also create a before vs. after table (Now vs. CalStart) showing how DueDate and AvailableDate differ?


````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Issue
Users expect a Scheduled Receipt (SR) at the sourcing site to fulfill the demand at the destination site. However, sometimes RapidResponse instead fulfills it with a Planned Order (PO).

Cause

The SR at the source is late, so RapidResponse generates a PO to meet the demand earlier (or reduce lateness).

Certain settings and configurations can make POs appear earlier/available than SRs.

Resolution / Key Settings to Check
To ensure SRs are prioritized correctly, review these factors:

OrderPolicy.PTFrule = LastDueLeadAll → ensures PO DueDate is later than SR.

SupplyStatus.IgnoreStartDate = Y → aligns lead times for PO & SR.

SupplyStatus.LeadTimeOverride = No → ensures equal lead time handling.

Lead time modifications → may cause POs to start earlier than SRs.

PartType.CommitLevel = Medium → gives SRs higher priority than POs.

PartType.AllocationQty = Ignore → prevents smaller POs from being favored over larger SRs.

AvailableDateLimit = Now (vs. Due) → avoids SRs being considered later than POs.

Allocation Table entries → can cause SR demand to explode to BOM components while PO demand doesn’t, delaying SR.

PartSource.BOMAlternate → must be valid (not “NoBom”) to avoid SR lateness.

MLS disabled → can also trigger earlier PO creation to reduce lateness.
`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
Issue
After upgrading from H2402.5 → H2412, Maestro no longer consumes OnHand (OH) and Scheduled Receipts (SR) from the same SubstituteGroup before creating PlannedPOs on the primary part.

In H2402.5 → no excess PlannedPOs.

In H2412 → ~160k PlannedPOs created unexpectedly, inflating buys and inventory positions.

Cause

The change is due to the new MLS Acceleration Feature (introduced in H2411+).

This feature improves runtime by sorting and evaluating supply sources based on historical performance, skipping weaker ones.

However, in this case it caused the MLS trial limit (750,000) to be hit early, leading to unexpected PlannedPO creation and undesired results.

Resolution / Workaround

Disable MLS Acceleration feature (requires Kinaxis Support if ≥ H2411).

Increase MLS Trial Limit in the MLS Configuration workbook to allow full evaluation.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Planned Transfer Allocation is generated when there is no Supply, this should not be happening.

Article Resolution
The observed behaviour is expected given the configuration.

Key Maestro functionality which explains the behaviour:

During the initialization of an MLS (Multi-Level Search) family, MLS sorts non-reschedulable scheduled receipts in priority order and then locks down their component and constraint usage. The highest priority scheduled receipts are processed first.
When a dependent demand is processed by MLS, this dependent demand is compared with all existing demands at the same level in the family. If the existing demand is preferred based on selection criteria like priority and date, that demand is processed before the new dependent demand. For parts using both expiry and safety stock, as in the customer example, priority comparisons are based on the part's default priority rather than the demand priority.
Reported Version

```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
How can Transit LeadTime be controlled when OnHand with the same Part Name are in different locations? If the Raw Part OnHand is located in Japan/US and the WIP FG Part is produced in Japan, the Transit Time will be very different and will greatly affect the DueDate of the Planned Orders.

Article Resolution
When parts with the same part name exist in different locations, different part site combinations need to be created in RapidReponse and they are essentially different parts.

 

If you have part A at site US and you also have Part A at site Japan, 2 parts need to be created in RapidResponse. When you have OH in two different sites, they should be populated into the OnHand table under different sites. If stock is produced in Japan and can be transferred to the US, then a Make part source for Part A at site Japan and a transfer part source at site US (transfer from site Japan) should be created. Both part sources should have Lead time defined (mostly transit time for transfer part source). Once defined, if a planned order (PO) is generated at site US using this transfer part source, then the transit lead time will be taken into account when calculating the DueDate and available date of the PO.

``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
Article Summary
RapidResponse is generating additional planned orders than required for expiry component parts. When the Internal to Expiry value is changed to a larger value, then no excess planned orders are generated.

 

Cause:
The ITE of the component part source is shorter than the ITE of the parent part, which caused the unexpected results.

At a high level, the child level usually needs to have a longer (or equal) ITE than the parent level, to account for manufacturing lead times and safety lead times (SLT). How to calculate ITE is included in the below screenshot.

https://knowledge.kinaxis.com/s/article/Additional-Planned-orders-are-generated-than-required-for-expiry-parts

for parent having single compoenent either through Make or Transfer
Parent Part ITE = ChildPart ITE - ChildPart's Safety Lead Time - ParentPartSource Leadtime    

for Parent having multiple components either through Make/transfer

Parent Part ITE = Min(Child Parts ITE) - Min(Child Parts Safety Leadtime) - ParentPartSource Leadtime

[To avoid excess planned orders for expiry-controlled parts, make sure the Intervals To Expiry (ITE) of child components is longer than (or at least equal to) the ITE of the parent part]

```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Detailed Guide: Planning Bi-Directional Transfers in RapidResponse
1. The Problem: Recursive Transfers

In supply chain, sometimes parts move back and forth between two or more sites.

Example: Site A → Site B for subassembly, then Site B → Site A for final assembly.

Business expectation: RapidResponse should plan these flows naturally.

Reality: Maestro flags this as Recursive PartSource, which planning algorithms cannot resolve (loops break netting and pegging logic).

This creates a gap: business needs ↔ Maestro limitations.

2. Workarounds to Model Bi-Directional Transfers
Method 1: Scheduled Receipt (SR) + Independent Demand (ID) Pair

When to use: Rare or occasional reverse transfers.

How it works:

You don’t actually create a recursive transfer PartSource.

Instead, you manually model the transfer as a demand and supply pair.

Example (Site B → Site A):

Put an Independent Demand (ID) at Site B on the ship-out date.

Represents the transfer leaving Site B.

Put a Scheduled Receipt (SR) at Site A on the receive date.

Represents the arrival at Site A.

Why it works:

Maestro sees it as a one-way transfer (ID and SR), not a recursive loop.

Demand pegging works properly.

Pros:

Simple and lightweight for infrequent transfers.

Easy to maintain if only a few cases.

Cons:

Not scalable. Becomes messy with many transfers.

Requires manual maintenance of ID/SR records.

Method 2: Virtual Site Approach

When to use: Frequent, regular, or multi-directional transfers across many sites.

Concept: Create a centralized virtual site (X) to manage shared inventory.

Instead of directly transferring between A ↔ B, you transfer through X.

X acts like a logical pooling site for excess inventory.

How it works:

Create Virtual Site X.

Run a process that calculates excess inventory at Sites A, B, C.

Move that excess into OnHand at Site X.

Tag the record with Location = original site (so you know where the stock physically is).

Example:

Part	Site	Location	Qty
P1	X	Site A	50
P1	X	Site B	75

When Site C has demand:

Maestro allocates demand to inventory in Site X.

Pegging report shows that the 10 units came from Site B’s bucket in Site X.

Then, an SR + ID pair is created for the actual physical transfer from Site B → Site C.

A scheduled batch process refreshes Site X inventory regularly.

Why it works:

Breaks the recursive loop because Maestro sees all supply flowing through X, not directly between sites.

Keeps planning consistent while still honoring real site ownership.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Planned Orders are not generated in the Planning Sheet workbook despite having demand.
 

Cause
Part is not valid after the Effective Out Date. The Part Source Effective Out Date is earlier than the Planning Time Fence (PTF) therefore no planned orders are generated.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Primary part-source still chosen at phase-out — planned orders pushed to “Future” because Netting consumed constraint at DueDate but CTP moved work later when components were late

Below I’ll explain exactly what happens, why it produces planned orders with AvailableDate = Future, and how to fix or mitigate it — with a concrete, step-by-step numeric example so the mechanism is obvious.

1) Short summary (one sentence)

Netting assigns planned orders to the primary part source and consumes that part-source’s constraint using the PO DueDate, but because BOM components arrive late the material start moves later during CTP — which causes those POs to actually consume constraint in later buckets (after the primary source phased out), leaving the POs tied to a source that is no longer available → POs end up with AvailableDate = Future.

2) Key concepts you must keep in mind

Netting creates planned orders and consumes constraint buckets based on the PO DueDate (preplanning step).

CTP (capacity / material reconciliation) runs afterwards and verifies material & capacity — if components are late it can move the actual start of a planned order to later buckets (MaterialStartDay).

EffectiveOut on a PartSource defines until when that source can be used. If the order’s real start moves past that EffectiveOut, the part source can no longer supply it.

If Netting already consumed constrained capacity for the primary source, and CTP later moves that work into buckets where the primary source is invalid/over-subscribed, you get a “domino” of reassignments and Future available dates.

3) Concrete numeric example (walkthrough)

Setup / dates

Part = P

Two PartSources:

PS1 (primary) — Effective: Aug 1, 2025 → Oct 31, 2025 (phases out end of October).

PS2 (secondary) — Effective: long term (through 2028).

Constraints:

Constraint1 (for PS1): capacity 200 units in September, 200 units in October (total 400 across the 2 months).

Constraint2 (for PS2): large capacity ongoing.

Demand (forecast) for P:

Sep demand = 200 units

Oct demand = 200 units

Nov demand = 200 units

BOM components (component Q) are late: their scheduled receipts make them actually available in mid-November for production that Netting assumed could run in October.

Netting run (early Sep)

Netting sees demand and available PS1 capacity for Sep/Oct and creates PlannedOrders:

PO_A (200 units) — DueDate Sep 20, assigned to PS1 (consumes Sep constraint 200/200).

PO_B (200 units) — DueDate Oct 20, assigned to PS1 (consumes Oct constraint 200/200).

At Netting time, both POs look OK because PS1 has capacity in Sep/Oct.

Later — when CTP runs and material lateness is discovered

The BOM component Q that PO_B needs is actually arriving late (component SR expected Nov 10). That means PO_B cannot start/finish in October — its material start shifts into November (MaterialStartDay = Nov 10).

During CTP, PO_B must now occupy November capacity, not October.

Constraint conflict and phase-out effect

But PS1 is only valid through Oct 31. So PO_B cannot legitimately be produced by PS1 in November.

Netting had already “spent” PS1 capacity for October — CTP now tries to steal November capacity for PO_B but PS1 is not available in November → the system cannot find PS1 capacity for the new (moved) start. The order is now effectively unfulfillable by PS1 at its needed time.

If the engine does not automatically reassign that PO to PS2 during CTP (or cannot because of bucket packing and preplanning logic), the PO shows up with AvailableDate = Future (no valid immediate supply from PS1); business ends up with POs that appear to be sourced from PS1 but are actually unservable until a future date or until re-sourced.

End result

POs that Netting preassigned to PS1 appear in the system but are effectively blocked (Future) because material lateness moved their start past PS1’s EffectiveOut and/or into already-used constraint buckets. This produces late/unusable planned buys and a “domino” of moved capacity.

4) Root cause (compact)

Timing mismatch between Netting (which consumes constraint at PO DueDate) and CTP (which may reassign start dates based on material availability).

Material lateness (MaterialStartDay > PlannedOrder.DueDate) causes the PO to consume later constraint buckets than Netting expected.

Primary part source phases out at the end of a bucket window, so the later bucket may have no PS1 capacity / PS1 not effective → planned order becomes Future instead of being produced by PS2.

Netting’s “prepacking” of buckets means upstream decisions (PS1 assignment) were made without the final material availability picture.


`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

BOM Analysis table is showing the following component twice with the only difference in the Earliest Demand Date & Last Demand Date. At the same time, another component 2500165 of the same assembly only has one record.

Article Resolution
This is occurring because if any part source is effective earlier than the primary part source, then it will be included in the Indented BOM report.

```````````````````````````````````````````````````````````````````````````````````````````````````````````

Part Source Selection Logic – Summary

Standard PartSource Selection

When inventory and receipts are exhausted, Netting creates planned orders from active sources.

Respects source allotment rules.

If attribute-based planning is used, only sources meeting attribute qualifications are considered.

If pooling logic is enabled, Netting chooses sources matching the demand pool (or with null pool).

Multi-Level Search (MLS) PartSource Selection

For parts with MLS enabled, decisions consider:

Source effectivity, planning time fence, and constraints on the assembly part.

Component constraints and availability.

Goal: ensure sourcing provides orders on time while using available component supplies before generating new orders.

Primary PartSource Selection

Primary source is determined by sorting PartSource records using:

Effective date validity within planning calendar.

Priority (ascending/descending).

Target value (descending).

First effective date.

Supplier ID.

If no valid PartSource exists → PrimaryPartSource = NULL.


````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Auto-Creation of Records Despite Auto Parameter = No – Summary

Issue: Records are still auto-created on a table even when the Auto parameter in Data Sources and Mapping is set to No.

Reason:

The table’s keys are references to other tables (no primary key of its own).

The Auto setting only prevents auto-creation of the referenced key fields, not the table records.

Example: For PartCustomer, it applies to Customer.Id, Part.Name, Part.Site references.

Resolution:

To fully stop record auto-creation, disable the Data Model option:
Uncheck Allow automatic record creation.

Path: Data Model → Right-click Table → Table Information → General tab.


`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Constraint Priority Not Respected – Summary

Issue:

RapidResponse is selecting a lower-priority constraint even though the primary constraint has available capacity.

Even when only part X is active on the primary constraint, the system still uses the secondary constraint.

Reason:

If part X @ site Y is part of an MLS family and the ShortTermExcess = Reduce feature is enabled:

A post-processing step adjusts CPO (Calculated Planned Order) due dates after MLS runs.

CPOs can shift their due dates within the effective range of the PartSource.

However, CPOs cannot switch to a different PartSource once created—even if the primary source has earlier capacity.

Resolution:

This is a limitation of the ReduceExcess feature.

```````````````````````````````````````````````````````````````````````````

Planned Order Generation Issue – Summary

Problem:
Planned Orders stop generating after a certain period (beyond 2 years from MRPDate), even though demands exist.

Part has valid Part Source (effective dates ok, order policy = Default, BOM effective).

Two source constraints linked; when the second constraint is removed, Planned Orders generate correctly.

Findings & Suggestions:

Constraint Availability

Issue may occur if system cannot find enough ConstraintAvailable for both constraints in the same time bucket.

Setting constraint type = LoadOnly or increasing constraint rates can allow planning.

Netting vs CTP

If Netting finds a usable PartSource but CTP fails, Planned Orders appear with “Future” availability date.

In this case, no usable PartSource is found during Netting, so orders are not created.

Calendars Check

Ensure both the Part PlanningCalendar and Constraint Calendars have valid dates extending beyond 2 years.

✅ Key Takeaway: Planned Orders may fail to generate due to constraint conflicts, lack of usable PartSource during Netting, or calendar horizon limits. Removing/adjusting constraints, changing to LoadOnly, or extending calendars can resolve the issue.

PlannedOrder. future date

``````````````````````````````````````````````````````````````````````````````````````````````````````````

Best Practices for Tackling Part & Part Source Chain Length

Issue:
High version chain lengths (Part = 23, Part Source = 59, Reference Part = 74) impacting system performance.
Each edit creates a new record version; long chains force the database to scan many versions, slowing performance and wasting memory.

Actions Already Taken:

Reduced historical snapshots.

Lowered scenario deletion threshold from 21 → 10 days.

Expert Recommendations:

Identify Data Churn Sources

Long chains often come from frequent changes to fields (often for reporting, not analytics).

Use tools/analysis to find which scenarios & fields cause the most churn.

Process & Data Redesign

Move frequently changing, non-analytical fields to a separate table and reference the Part table.

Use Embedded Algorithms or caching to handle references efficiently.

Scenario Management

Continue pruning old/unused scenarios.

Use scenario perspectives instead of editing large volumes of records to simulate changes.

Engage Kinaxis Support

Performance team can analyze and pinpoint root causes of excessive chain lengths.

``````````````````````````````````````````````````````````````````````````````````````````````````````````

Changing Effective Lead Time Calculation for Transfer Parts – Summary

Issue:

Contract manufacturers are modeled as separate sites in RR.

Parts are supplied via transfer PartSources.

Effective Lead Time (EffLeadTime) for transfer PartSources only includes Transit + Pre-Ship Lead Times.

Fixed/variable manufacturing lead times are ignored at transfer type, since lead time starts from the build date.

This results in underestimation (e.g., 11 days vs actual 47 days including manufacturing).

Requirement:

Keep PartSources as Transfer type.

Add part-specific manufacturing lead times into the effective lead time calculation.

Without changing source-level data (since multiple transfer parts share the same source).

Discussion & Suggestions:

Use Part.MultiSiteCumLeadTime

Captures lead times for transfer parts + components.

Useful for reporting, but doesn’t change planning behavior.

Adjust Dock-to-Stock Lead Time

Works at PartSource level, allowing differentiation across parts from the same source.

Requires integration effort to populate.

Use Planning Time Fence (PTF) Workaround

Populate PlanningTimeFence = (Manufacturing + Transit + Pre-ship lead times).

Set OrderPolicy.PTFRule = Fence.

Prevents planned orders from being created earlier than this horizon, effectively simulating the longer lead time.

Limitation: If supply already exists at the sending site, PTF still applies, which may delay unnecessarily.

Conclusion:

Native RR behavior does not allow including manufacturing lead time in transfer EffLeadTime.

Best workarounds:

Dock-to-Stock lead time at PartSource level (cleaner, but integration needed).

Planning Time Fence approach (no input change, but less precise).

True solution would require enhancement to RR logic.


✅ Key Takeaway: RR does not natively support adding manufacturing lead time into transfer EffLeadTime. Use Dock-to-Stock (preferred, part-level control) or Planning Time Fence as workarounds, or request a product enhancement.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````

Include non-primary part sources in FlatBillDown and FlatBillUp calculations
The calculated FlatBillDown and FlatBillUp tables report the results of explosions through BOM records. The FlatBillDown table reports all components used to make a given part, while the FlatBillUp table reports all assemblies where a given part is used.

By default, the records generated in the FlatBillDown and FlatBillUp tables only contain results applicable to the primary part source for a given part. This means if an assembly has multiple part sources, the FlatBillDown table only reports details of components required to build the assembly when sourced from the primary part source. It does not, however, report details of component requirements when sourcing the part from other potential sources. For example, if a non-primary part source is associated with an alternate BOM, the results of exploding that alternate BOM would not be reported in the FlatBillDown table. Similarly, if an assembly has global part substitutes, the FlatBillDown table does not report details of those substitute parts or their components.

If any non-primary part sources should be included when generating FlatBillDown and FlatBillUp records, the RR_Analytics_FlatBill_MultiSourceExplode algorithm parameter can be used. This parameter can be defined as a workbook boolean variable, and applied to any worksheets based on the FlatBillDown or FlatBillUp tables within that workbook. Valid options for this parameter/variable are:

False: Only results from the primary part source for a given part are reported. This setting corresponds to the behavior seen when not using the parameter/variable. For example, for a given assembly, the FlatBillDown table would report only those components required when the assembly was sourced strictly from the primary part source.
True: Results from all eligible part sources for a given part are reported. For example, for a given assembly, the FlatBillDown table would include records for components found from all eligible part sources (and not just the primary). Note also that if the assembly had global substitutes, then those substitutes along with all of their components would be reported (if just the substitutes should be included but their components excluded from FlatBillDown reports, then the RR_Analytics_FlatBill_AlternatePartExplode parameter can be used as discussed in Include global substitutes and their components in FlatBillDown calculations).

`````````````````````````````````````````````````````````````````````````````````````````````````````````

Primary Part Source in RapidResponse – Summary

Question:
How does RR determine the PrimaryPartSource? After importing Part & PartSource data, RR sometimes selects a PrimaryPartSource based on unclear (and seemingly incorrect) criteria.

Discussion:

Issues often arise during data import when references (e.g., Source.Supplier.Id) are missing or invalid.

If references cannot be resolved, some PartSource records are rejected during the update.

This can lead to RR picking the "wrong" PartSource as primary because valid records are missing.

Troubleshooting Steps:

Check Data Update Log

Errors & Warnings worksheet lists rejected PartSource records and missing references.

Only the first x errors are shown per table (default = 200), so some errors may be hidden.

Can adjust limits in Error Limits by Table worksheet under System Monitoring → Data Import & Update Log.

Common Cause

Missing or incorrect Source.Supplier.Id (or other reference fields like Source.ID, DestinationSite, etc.).

Example error:

Unable to resolve reference for "PartSource.Source" with values 
"Source.ID='No_Source_ZZ', Source.Supplier.Id='05-100' ..."


Once valid supplier/source data was provided in extracts, the issue was resolved.

Conclusion:

PrimaryPartSource determination depends on valid, effective PartSource records.

If references are missing, RR may fall back to another PartSource (appearing “wrong”).

Fix: ensure all PartSource imports have correct and consistent Supplier/Source references.

✅ Key Takeaway: RR does not arbitrarily choose the PrimaryPartSource — it selects from the valid, effective PartSource records. Import errors (like missing Source.Supplier.Id) can cause RR to reject the intended primary source and default to another.


```````````````````````````````````````````````````````````````````````````````````````````````````````````````````


SourceRule Table – Summary

Purpose:
Defines how PartSource values are interpreted when multiple part sources can satisfy a demand (splits, priorities, effectivity rules).
Referenced from Part.SourceRule.

Key Fields:

AllotmentRule – Determines how requirements are sourced when multiple PartSources have the same priority:

Ongoing (default) → allocate to source furthest below target.

OngoingToDate → allocate to source with lowest quota rating so far.

Proportional → split requirement by target ratios.

ProportionalLotSize → proportional but re-applied after each lot-sized allocation.

DateEffectivityRule – Handles cases when demand date falls outside PartSource effective window:

AdjustedDemandDate → demand left unsatisfied.

Normal (default) → supply still planned, adjusted to within effectivity range.

PriorityRule – Defines how priorities are interpreted:

Ascending (default) → lower number = higher priority.

Descending → higher number = higher priority.

ControlSet – Linked control set.

Description – Optional notes.

Value – Identifier for the SourceRule.

Parts – Parts referencing this SourceRule.

✅ In essence: SourceRule controls how RapidResponse decides which PartSource(s) to use when multiple are valid — covering priority handling, percentage splits, effectivity, and allotment logic.

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Safety Stock: DemandTransformation:

The DemandTransformationRule setting controls how priorities flow through supply planning when safety stock is involved. It basically answers:
👉 When a supply is created because of safety stock, should it "inherit" the priority of a future real demand it will eventually serve, or should it keep the lower/default priority of safety stock?


1. FirstInFirstOut

Logic: Supply created for safety stock inherits the priority of the actual demand it will end up serving (based on due date order).

Effect:

If a safety stock triggers planned supply today, and tomorrow a high-priority sales order arrives, the earlier supply is treated as if it was created for that sales order.

Supplies can split to match multiple demands of different priorities.

Prevents situations where high-priority orders get delayed just because the system thought supply was "for safety stock only."

When useful: If you want your supply chain to always reflect real demand priorities as soon as they appear, even if supply was pre-built for safety stock.

Example:

Safety stock requires 100 units. System creates supply for 100 (low priority).

Next day, a high-priority sales order of 50 comes in.

With FirstInFirstOut, the supply for 100 "transforms" — 50 gets assigned to the high-priority sales order, and the rest may stay low-priority.
➡️ The high-priority demand is satisfied on time.

2. Ignore

Logic: Supply created for safety stock keeps its default priority (usually low).

Effect:

Real demands that later consume this supply may be of higher priority, but that priority doesn’t propagate back.

This can cause issues where a lower-priority safety stock supply is used for a high-priority order, and the system doesn’t fully adjust priorities downstream.

When useful: If you want to keep things simple and treat safety stock as its own demand, separate from customer demand.

Example:

Safety stock requires 100 units → supply for 100 (low priority).

Next day, a high-priority sales order of 50 comes in.

With Ignore, the system still treats that supply as low priority.
➡️ The high-priority demand might get delayed if there are conflicts.

3. Default Behavior

Default = Ignore.

If CommitLevel = Medium or Low, system always uses default priority (ignores transformation), no matter what.

✅ In short:

FirstInFirstOut → safety stock supply "adapts" to real demand priorities (more flexible, prevents high-priority lateness).

Ignore → safety stock supply stays low priority (simpler, but risks late fulfillment for high-priority orders).




`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Why would Maestro create 2 records for dynamic safety stock calculation?

Article Resolution
From the Data model guide:

 

For parts using range of coverage calculations, an additional safety stock demand with quantity given by Part.SafetyStockQty is generated on the date indicated by Part.SafetyStockPolicy.SafetyStockDateRule. To avoid having this additional demand generated, ensure the value of Part.SafetyStockQty is set to 0.

``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````


Problem: Why does RapidResponse generate excess stock when using time-phased safety stock?

Root cause:

The part has high demand before the Planning Date Fence (PDFDate).

After the PDFDate, demand drops sharply (e.g., product is phasing out).

RapidResponse tries to protect against those early high demands by creating large planned orders.

But because of the Planning Time Fence (PTF), those orders can only be placed on the PDFDate (not earlier).

This creates a big lump of supply on the PDFDate.

After the PDFDate, safety stock decreases (because demand drops), but since demand falls even faster, the extra stock never gets consumed.

Result = excess stock remains in the system.

Resolution / Fix

Recheck Safety Stock settings for parts with phasing-out demand.

Example: Shift the start date of safety stock to the PTFDate instead of earlier.

Consider MultipleQty rules:

If demand later is small, large multiples may create leftover stock that looks like excess.


````````````````````````````````````````````````````````````````````````````````````````````````````````````
In the 'Errors and Warnings' worksheet of the 'Data Import and Update log', an error appears stating that a duplicate record was ignored due to matching key values. This happens when multiple records in a data update have the same key, possibly due to:

Duplicates in a single update file

Duplicates across multiple data sources (e.g., SAP\Part.tab and SAP_ECC\Part.tab)

Resolution Steps:

Review the error details to locate the duplicate record and its key values.

Identify the key fields using Data Source and Mapping information.

Remove duplicate records from the extract file(s).

Run each Data Source in separate update tasks if duplicates exist across sources.

Re-run the update after cleaning the data.

Investigate the source system to prevent future extraction of duplicate records.

``````````````````````````````````````````````````````````````````````````````````````````````````````

Update worksheet filters to use Reviewed = 'N' instead of Reviewed = FALSE.

``````````````````````````````````````````````````````````````````````````````````````````````````````

push planning , DRP, Distribution planning


🔎 What the Article Says in Simple Terms

The issue:

The system respects the lot size rule in total pushed quantity per day, but the individual push transfer orders are split oddly and don’t individually align with min/max/multiple lot size rules.

Users expect aggregated transfers that directly match lot sizes.

Instead, they see multiple transfers with "broken" quantities like decimals or weird numbers.

The root cause:

Push planning goes through three main splitting stages, which introduce these unusual numbers.

The final pushed total (for that day) still respects the lot size rule, but the intermediate splits don’t.

⚙️ The 3 Splitting Stages
1. Final Constraint Consumption

Configuration:

ConstraintMinimumRule / ConstraintMultipleRule = MultiplePeriods

Effect:

Scheduled receipts (SRs) and planned orders are split across multiple periods.

Instead of neat integers, the system may generate decimal quantities (e.g., 10973.95 instead of 10974).

👉 Example:
A planned order for 10,974 could be split as 10,973.05 + 0.95, due to period-based allocation.

2. Supply Allotment to Demand for Push Parts

Configuration:

part.AllocationMultiple = 1

Effect:

Supplies that already have decimals are further split into:

A whole number portion

A fractional portion (<1)

👉 Example:
10973.95 gets split into:

10973 (whole)

0.95 (fractional)

🔑 Proof: When the customer tested with part.AllocationMultiple = 0, the fractional (<1) push transfers disappeared.
So, this stage is the reason why you sometimes see tiny orders like 0 or 1 unit transfers.

3. Push Transfer Stage

This is where Maestro decides how much to push out of the supply.

Rules applied here:

Don’t push more than excess supply.

Push in multiples of the parent MultipleQty lot size.

If needed, it splits supplies further to respect these conditions.

👉 Example:

Excess supply = 20,736 (on July 4, 2025).

First supply processed = 10,973.95.

System wants to push multiples of parent lot size.

Rounded down multiple → 10,364.

This required 3 splits:

0.95

10,363.05

605.95

Total = 10,973.95 ✅

So, while the total is correct and respects multiples, the individual transfers look odd.

📊 Why You See Weird Quantities like 0, 1, 384, 606

0 or 1 → Comes from fractional split stage (AllocationMultiple = 1).

384, 606, etc. → Comes from the push transfer stage where Maestro rounds to multiples and adjusts leftovers.

Essentially:

Decimal → split into whole + fractional

Lot size rounding → creates uneven remainders

Each remainder becomes its own push transfer

So the "broken" looking transfers are just the byproduct of enforcing lot size + multiple + minimum rules across multiple stages.

✅ Key Takeaways

The total pushed per day always respects lot size (MultipleQty).

The individual transfers may not, because they’re just split representations of original supplies.

The splits happen due to:

Constraint period splitting → decimals

AllocationMultiple = 1 → whole vs fractional split

Push transfer rules → rounding & remainder splits

If you want fewer odd splits, you can test with:

AllocationMultiple = 0 (removes fractional transfers).

Adjusting constraint rules to reduce decimals.

👉 Example in plain terms:
It’s like you want to ship boxes of 12 bottles (lot size = 12).

First, your stock is split into 12.95 bottles (weird decimal).

Then, it’s further split into 12 + 0.95.

Then, when pushing, the system only ships multiples of 12, so it ships 12, leaving a 0.95 remainder.

Result: You see shipments of 12 and 0.95 — but total shipped is correct.

`````````````````````````````````````````````````````````````````````````````````````````````````````

The sorting criteria for Onhand in CTP are as follows:

 

1. If the part is subject to expiry: Estimated Expiry Date

2. If the part does not expire: Onhand Date

3. Unit Cost

4. Quantity

5. Location

6. Warehouse

`````````````````````````````````````````````````````````````````````````````````````````````````````

Article Summary
Access to the Heartbeat logs within the Embedded Algorithm Details and Log is required.

Article Resolution
At this time, customers will not be grated access to the heartbeat logs, their use in intended for Kinaxis teams only.

The Embedded Algorithm logs will be available to Kinaxis customers from Maestro version SU 2508.

````````````````````````````````````````````````````````````````````````````````````````````````````

Kinaxis acronym list, short forms
https://knowledge.kinaxis.com/s/kinaxis-acronym-list?t=1549900472597

```````````````````````````````````````````````````````````````````````````````````````````````````````



Article Summary
Data changes made within scenarios are not being tracked as expected. As a result, the following worksheets in the Scenario Properties window appear blank:

Data Changes (Pending Commits)
Pending Updates
Pending Update Conflicting Changes
Inherited Data Changes
 

This issue prevents users from viewing and managing data change activity within affected scenarios.

 

Cause
Here are some limitations that would explain why the changes are not being tracked

Information is displayed on the Pending Updates and Pending Update Conflicting Changes tabs only if you have access to the scenario's parent.
In most cases, data changes made using automated processes, such as workbook commands, aren't listed in the Data Changes (Pending Commits) tab. If automated processes have been used to update data, a summary of the changes appears in the activity log (available on the General tab of the scenario properties).
Changes older than a certain age (45 days by default, but this number can be changed by an administrator) are not shown on the Data Changes (Pending Commits), Pending Updates, or Inherited Data Changes tabs. Conflicting changes are always shown no matter when the changes were made.
These tabs might not be able to display data for a scenario with thousands of changes, due to memory limits. In this case, your Maestro Administrator can adjust the limit.
Changes to tables containing vector set data aren't tracked.
 

Another reason may be the automatic deletion of Data Change Records has been disabled, which would automatically disable change tracking.

```````````````````````````````````````````````````````````````````````````````````````

Article Summary
Unable to track the data changes made using a DataChange_Local table based worksheet. While the results are achievable using DataChange table based worksheet.

 

Cause
The DataChange_Local store the records for the data changes made in the specific scenario while the DataChange table store the records for data changes made in the specific scenario, data changes commit from the its child scenario, or data change made from its parent scenario. 

````````````````````````````````````````````````````````````

Article Summary
Unable to track the data changes made using a DataChange_Local table based worksheet. While the results are achievable using DataChange table based worksheet.

 

Cause
The DataChange_Local store the records for the data changes made in the specific scenario while the DataChange table store the records for data changes made in the specific scenario, data changes commit from the its child scenario, or data change made from its parent scenario. 

````````````````````````````````````````````````````````````````````

Administrators noted that inserts and deletes performed by workbook commands were missing in the DataChange table.


Cause
By default, workbook commands do not capture insert or delete operations unless the Log data changes option is enabled.
Without this setting, only specific modify actions may be recorded in the DataChange table.


Article Resolution
Enabling Log data changes for workbook commands ensures that inserts, deletes, and modifies are tracked in the DataChange table.
By enabling this checkbox, Maestro will be placed under additional performance stress that may negatively impact overall system performance.

```````````````````````````````````````````````````````````````````````````````````````````````````













